{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e614bfb",
   "metadata": {},
   "source": [
    "# MNLI 데이터셋 분석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31ade6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import numpy\n",
    "import transformers\n",
    "import argparse\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8365fb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset glue (/aiffel/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c52acbaad549fdb07a0484d8763cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFRobertaForSequenceClassification\n",
    "from transformers.data.processors.utils import InputExample\n",
    "from datasets import load_dataset\n",
    "\n",
    "# STEP 1: MNLI 데이터셋 분석하기 및 데이터셋 로드\n",
    "# Huggingface의 datasets 라이브러리를 사용하여 MNLI 데이터셋을 로드합니다.\n",
    "dataset = load_dataset('glue', 'mnli')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbd98a6",
   "metadata": {},
   "source": [
    "# MNLIProcessor클래스 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "453f8dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNLIProcessor:\n",
    "    \"\"\"Processor for the MNLI dataset using Huggingface datasets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, dataset):\n",
    "        \"\"\"Returns training examples.\"\"\"\n",
    "        return self._create_examples(dataset['train'], \"train\")\n",
    "\n",
    "    def get_dev_examples(self, dataset):\n",
    "        \"\"\"Returns dev examples.\"\"\"\n",
    "        return self._create_examples(dataset['validation_matched'], \"dev\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Returns possible labels.\"\"\"\n",
    "        return [\"entailment\", \"contradiction\", \"neutral\"]\n",
    "\n",
    "    def _create_examples(self, dataset_split, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for i, example in enumerate(dataset_split):\n",
    "            guid = f\"{set_type}-{i}\"\n",
    "            text_a = example['premise']\n",
    "            text_b = example['hypothesis']\n",
    "            label = str(example['label'])\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7403ecc8",
   "metadata": {},
   "source": [
    "# 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d0e8363",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "processor = MNLIProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ee591c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 예제 가져오기\n",
    "train_examples = processor.get_train_examples(dataset)\n",
    "label_list = processor.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d741576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ 원본 데이터 ------\n",
      "InputExample(guid='train-0', text_a='Conceptually cream skimming has two basic dimensions - product and geography.', text_b='Product and geography are what make cream skimming work. ', label='1')\n",
      "------ Processor 가공 데이터 ------\n",
      "InputExample(guid='train-0', text_a='Conceptually cream skimming has two basic dimensions - product and geography.', text_b='Product and geography are what make cream skimming work. ', label='1')\n"
     ]
    }
   ],
   "source": [
    "# 단위 테스트: Processor 클래스가 올바르게 동작하는지 확인\n",
    "def test_processor(processor, dataset):\n",
    "    examples = processor.get_train_examples(dataset)\n",
    "    assert len(examples) > 0, \"Processor returned an empty list of examples\"\n",
    "\n",
    "    example = examples[0]\n",
    "    print(\"------ 원본 데이터 ------\")\n",
    "    print(example)\n",
    "    \n",
    "    processed_example = processor._create_examples(dataset['train'], \"train\")[0]\n",
    "    print(\"------ Processor 가공 데이터 ------\")\n",
    "    print(processed_example)\n",
    "\n",
    "# Processor 단위 테스트 실행\n",
    "test_processor(processor, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb7e182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_features(example, label_list, max_length, tokenizer):\n",
    "    \"\"\"하나의 InputExample을 BERT가 이해할 수 있는 features로 변환합니다.\"\"\"\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        example.text_a,\n",
    "        example.text_b,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    \n",
    "    label = int(example.label)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"label\": label\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e11cad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(examples, label_list, max_length, tokenizer, batch_size):\n",
    "    \"\"\"features를 tf.data.Dataset 형태로 변환합니다.\"\"\"\n",
    "    features = [convert_example_to_features(example, label_list, max_length, tokenizer) for example in examples]\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f[\"input_ids\"],\n",
    "                    \"attention_mask\": f[\"attention_mask\"],\n",
    "                },\n",
    "                f[\"label\"],\n",
    "            )\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({\n",
    "            \"input_ids\": tf.int32,\n",
    "            \"attention_mask\": tf.int32,\n",
    "        }, tf.int64),\n",
    "        ({\n",
    "            \"input_ids\": tf.TensorShape([None]),\n",
    "            \"attention_mask\": tf.TensorShape([None]),\n",
    "        }, tf.TensorShape([])),\n",
    "    )\n",
    "\n",
    "    dataset = dataset.padded_batch(batch_size, padded_shapes=({\n",
    "        \"input_ids\": [max_length],\n",
    "        \"attention_mask\": [max_length],\n",
    "    }, []))\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bc27c4",
   "metadata": {},
   "source": [
    "# 모델 생성 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50a2ff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = processor.get_train_examples(dataset)[:len(dataset['train']) // 10]\n",
    "num_train_examples = len(train_examples)\n",
    "\n",
    "# steps_per_epoch를 계산\n",
    "batch_size = 32\n",
    "steps_per_epoch = num_train_examples // batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b36fdb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 로버타 모델 로드 및 컴파일\n",
    "model = TFRobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "598bcf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터셋 생성 및 캐싱, prefetch 적용\n",
    "train_dataset = create_dataset(train_examples, label_list, max_length=128, tokenizer=tokenizer, batch_size=batch_size)\n",
    "train_dataset = train_dataset.cache().prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bb0c146",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6324c14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1227/1227 [==============================] - 992s 797ms/step - loss: 0.6111 - accuracy: 0.7475\n",
      "Epoch 2/3\n",
      "   1/1227 [..............................] - ETA: 3:57 - loss: 0.9885 - accuracy: 0.6667WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3681 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3681 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1227/1227 [==============================] - 0s 210us/step - loss: 0.9885 - accuracy: 0.6667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7c9c24a38b80>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습\n",
    "model.fit(train_dataset, epochs=3, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37e33a6",
   "metadata": {},
   "source": [
    "# 모델 평가 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f32f474a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307/307 [==============================] - 89s 279ms/step - loss: 0.4563 - accuracy: 0.8205\n",
      "Evaluation Loss: 0.4563239812850952\n",
      "Evaluation Accuracy: 0.8204788565635681\n"
     ]
    }
   ],
   "source": [
    "# 검증 데이터셋 생성\n",
    "dev_examples = processor.get_dev_examples(dataset)\n",
    "dev_dataset = create_dataset(dev_examples, label_list, max_length=128, tokenizer=tokenizer, batch_size=32)\n",
    "\n",
    "# 모델 평가\n",
    "eval_results = model.evaluate(dev_dataset)\n",
    "print(f\"Evaluation Loss: {eval_results[0]}\")\n",
    "print(f\"Evaluation Accuracy: {eval_results[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
